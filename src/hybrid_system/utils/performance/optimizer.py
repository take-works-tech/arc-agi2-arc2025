#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ï¼ˆæ–°ã‚·ã‚¹ãƒ†ãƒ çµ±åˆç‰ˆï¼‰
å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã¨æŽ¨è«–ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ã‚’æœ€é©åŒ–
"""

import numpy as np
import json
import os
import time
import psutil
import torch
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, asdict
from pathlib import Path

from src.hybrid_system.utils.logging import Logger

logger = Logger.get_logger("PerformanceOptimizer")

@dataclass
class PerformanceMetrics:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    execution_time: float
    memory_usage: float  # MB
    cpu_usage: float  # %
    gpu_usage: float = 0.0  # MB
    accuracy: float = 0.0
    throughput: float = 0.0  # ops/sec

@dataclass
class OptimizationResult:
    """æœ€é©åŒ–çµæžœ"""
    component: str
    before_metrics: PerformanceMetrics
    after_metrics: PerformanceMetrics
    improvement_ratio: float
    optimization_applied: str
    success: bool

class PerformanceOptimizer:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æœ€é©åŒ–å™¨"""

    def __init__(self, results_dir: str = "performance_optimization_results"):
        """åˆæœŸåŒ–"""
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(parents=True, exist_ok=True)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self._function_cache: Dict[int, Any] = {}
        self._cache_hits = 0
        self._cache_misses = 0

        logger.info("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æœ€é©åŒ–å™¨åˆæœŸåŒ–å®Œäº†")

    def measure_performance(self, func: Callable, *args, **kwargs) -> PerformanceMetrics:
        """
        é–¢æ•°ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ã‚’æ¸¬å®š

        Args:
            func: æ¸¬å®šã™ã‚‹é–¢æ•°
            *args, **kwargs: é–¢æ•°ã®å¼•æ•°

        Returns:
            PerformanceMetrics: æ¸¬å®šçµæžœ
        """
        try:
            # ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±
            process = psutil.Process()

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®šé–‹å§‹
            memory_before = process.memory_info().rss / 1024 / 1024  # MB

            # CPUä½¿ç”¨çŽ‡æ¸¬å®šé–‹å§‹
            cpu_before = psutil.cpu_percent(interval=0.1)

            # GPUä½¿ç”¨çŽ‡æ¸¬å®šé–‹å§‹
            gpu_before = 0.0
            if torch.cuda.is_available():
                gpu_before = torch.cuda.memory_allocated() / 1024 / 1024  # MB

            # å®Ÿè¡Œæ™‚é–“æ¸¬å®š
            start_time = time.time()
            result = func(*args, **kwargs)
            execution_time = time.time() - start_time

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®šçµ‚äº†
            memory_after = process.memory_info().rss / 1024 / 1024  # MB
            memory_usage = max(0, memory_after - memory_before)

            # CPUä½¿ç”¨çŽ‡æ¸¬å®šçµ‚äº†
            cpu_after = psutil.cpu_percent(interval=0.1)
            cpu_usage = (cpu_before + cpu_after) / 2

            # GPUä½¿ç”¨çŽ‡æ¸¬å®šçµ‚äº†
            gpu_usage = 0.0
            if torch.cuda.is_available():
                gpu_after = torch.cuda.memory_allocated() / 1024 / 1024  # MB
                gpu_usage = max(0, gpu_after - gpu_before)

            # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—
            throughput = 1.0 / execution_time if execution_time > 0 else 0.0

            return PerformanceMetrics(
                execution_time=execution_time,
                memory_usage=memory_usage,
                cpu_usage=cpu_usage,
                gpu_usage=gpu_usage,
                throughput=throughput
            )

        except Exception as e:
            logger.error(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¸¬å®šã‚¨ãƒ©ãƒ¼: {e}")
            return PerformanceMetrics(
                execution_time=0.0,
                memory_usage=0.0,
                cpu_usage=0.0,
                gpu_usage=0.0,
                throughput=0.0
            )

    def optimize_with_cache(self, func: Callable, cache_size: int = 100) -> Callable:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¦é–¢æ•°ã‚’æœ€é©åŒ–

        Args:
            func: æœ€é©åŒ–ã™ã‚‹é–¢æ•°
            cache_size: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚º

        Returns:
            æœ€é©åŒ–ã•ã‚ŒãŸé–¢æ•°
        """
        def cached_func(*args, **kwargs):
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
            try:
                # argsã‚’ãƒãƒƒã‚·ãƒ¥åŒ–ï¼ˆnumpyé…åˆ—å¯¾å¿œï¼‰
                key_parts = []
                for arg in args:
                    if isinstance(arg, np.ndarray):
                        key_parts.append(hash(arg.tobytes()))
                    else:
                        key_parts.append(hash(str(arg)))
                cache_key = hash(tuple(key_parts))
            except:
                # ãƒãƒƒã‚·ãƒ¥åŒ–å¤±æ•—ã®å ´åˆã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—
                return func(*args, **kwargs)

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            if cache_key in self._function_cache:
                self._cache_hits += 1
                return self._function_cache[cache_key]

            # é–¢æ•°å®Ÿè¡Œ
            self._cache_misses += 1
            result = func(*args, **kwargs)

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆã‚µã‚¤ã‚ºç®¡ç†ï¼‰
            if len(self._function_cache) >= cache_size:
                # æœ€ã‚‚å¤ã„ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤ï¼ˆLRUï¼‰
                oldest_key = next(iter(self._function_cache))
                del self._function_cache[oldest_key]

            self._function_cache[cache_key] = result
            return result

        return cached_func

    def compare_implementations(
        self,
        component_name: str,
        original_func: Callable,
        optimized_func: Callable,
        test_data: Any,
        optimization_description: str
    ) -> OptimizationResult:
        """
        2ã¤ã®å®Ÿè£…ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ã‚’æ¯”è¼ƒ

        Args:
            component_name: ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå
            original_func: å…ƒã®å®Ÿè£…
            optimized_func: æœ€é©åŒ–ç‰ˆã®å®Ÿè£…
            test_data: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
            optimization_description: æœ€é©åŒ–ã®èª¬æ˜Ž

        Returns:
            OptimizationResult: æ¯”è¼ƒçµæžœ
        """
        logger.info(f"{component_name}ã®æœ€é©åŒ–é–‹å§‹")

        # å…ƒã®å®Ÿè£…ã‚’æ¸¬å®š
        before_metrics = self.measure_performance(original_func, test_data)

        # æœ€é©åŒ–ç‰ˆã‚’æ¸¬å®š
        after_metrics = self.measure_performance(optimized_func, test_data)

        # æ”¹å–„çŽ‡è¨ˆç®—
        if before_metrics.execution_time > 0:
            improvement_ratio = (
                (before_metrics.execution_time - after_metrics.execution_time) /
                before_metrics.execution_time
            )
        else:
            improvement_ratio = 0.0

        result = OptimizationResult(
            component=component_name,
            before_metrics=before_metrics,
            after_metrics=after_metrics,
            improvement_ratio=improvement_ratio,
            optimization_applied=optimization_description,
            success=improvement_ratio > 0
        )

        logger.info(f"{component_name}æœ€é©åŒ–å®Œäº†: æ”¹å–„çŽ‡ {improvement_ratio:.3f}")
        return result

    def optimize_memory_usage(self, func: Callable) -> Callable:
        """
        ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æœ€é©åŒ–

        Args:
            func: æœ€é©åŒ–ã™ã‚‹é–¢æ•°

        Returns:
            æœ€é©åŒ–ã•ã‚ŒãŸé–¢æ•°
        """
        def optimized_func(*args, **kwargs):
            # å®Ÿè¡Œå‰ã«ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
            import gc
            gc.collect()

            # GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            # é–¢æ•°å®Ÿè¡Œ
            result = func(*args, **kwargs)

            # å®Ÿè¡Œå¾Œã«ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            return result

        return optimized_func

    def save_optimization_results(self, results: Dict[str, OptimizationResult]):
        """æœ€é©åŒ–çµæžœã‚’ä¿å­˜"""
        logger.info("æœ€é©åŒ–çµæžœä¿å­˜é–‹å§‹")

        # çµæžœã‚’è¾žæ›¸å½¢å¼ã«å¤‰æ›
        results_data = {}
        for component, result in results.items():
            results_data[component] = {
                'optimization_applied': result.optimization_applied,
                'improvement_ratio': result.improvement_ratio,
                'success': result.success,
                'before_metrics': asdict(result.before_metrics),
                'after_metrics': asdict(result.after_metrics)
            }

        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        results_file = self.results_dir / "optimization_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, ensure_ascii=False, indent=2)

        # ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
        self._generate_optimization_report(results)

        logger.info(f"æœ€é©åŒ–çµæžœä¿å­˜å®Œäº†: {results_file}")

    def _generate_optimization_report(self, results: Dict[str, OptimizationResult]):
        """æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        report_file = self.results_dir / "optimization_report.txt"

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ\n")
            f.write("=" * 60 + "\n\n")

            f.write(f"æœ€é©åŒ–å®Ÿè¡Œæ—¥æ™‚: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")

            f.write("ã€æœ€é©åŒ–ã‚µãƒžãƒªãƒ¼ã€‘\n")
            total_improvements = 0
            successful_optimizations = 0

            for component, result in results.items():
                f.write(f"\n{component.upper()}:\n")
                f.write(f"  æœ€é©åŒ–æ‰‹æ³•: {result.optimization_applied}\n")
                f.write(f"  æ”¹å–„çŽ‡: {result.improvement_ratio:.3f}\n")
                f.write(f"  æˆåŠŸ: {'ã¯ã„' if result.success else 'ã„ã„ãˆ'}\n")

                if result.success:
                    total_improvements += result.improvement_ratio
                    successful_optimizations += 1

                before = result.before_metrics
                after = result.after_metrics
                f.write(f"  å®Ÿè¡Œæ™‚é–“: {before.execution_time:.4f}s -> {after.execution_time:.4f}s\n")
                f.write(f"  ãƒ¡ãƒ¢ãƒª: {before.memory_usage:.2f}MB -> {after.memory_usage:.2f}MB\n")
                f.write(f"  CPU: {before.cpu_usage:.2f}% -> {after.cpu_usage:.2f}%\n")
                if before.gpu_usage > 0 or after.gpu_usage > 0:
                    f.write(f"  GPU: {before.gpu_usage:.2f}MB -> {after.gpu_usage:.2f}MB\n")

            f.write(f"\nã€å…¨ä½“ã‚µãƒžãƒªãƒ¼ã€‘\n")
            f.write(f"  æˆåŠŸã—ãŸæœ€é©åŒ–æ•°: {successful_optimizations}/{len(results)}\n")
            avg_improvement = total_improvements / max(successful_optimizations, 1)
            f.write(f"  å¹³å‡æ”¹å–„çŽ‡: {avg_improvement:.3f}\n")

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ
            if self._cache_hits + self._cache_misses > 0:
                cache_hit_rate = self._cache_hits / (self._cache_hits + self._cache_misses)
                f.write(f"  ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆçŽ‡: {cache_hit_rate:.3f}\n")

            if successful_optimizations == len(results):
                f.write(f"  æœ€é©åŒ–çŠ¶æ³: å…¨ã¦æˆåŠŸ âœ…\n")
            elif successful_optimizations > len(results) // 2:
                f.write(f"  æœ€é©åŒ–çŠ¶æ³: å¤§éƒ¨åˆ†æˆåŠŸ ðŸŸ¡\n")
            else:
                f.write(f"  æœ€é©åŒ–çŠ¶æ³: æ”¹å–„ãŒå¿…è¦ ðŸ”´\n")

        logger.info(f"æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_file}")

    def get_cache_statistics(self) -> Dict[str, Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚’å–å¾—"""
        total_requests = self._cache_hits + self._cache_misses
        hit_rate = self._cache_hits / total_requests if total_requests > 0 else 0.0

        return {
            'cache_size': len(self._function_cache),
            'cache_hits': self._cache_hits,
            'cache_misses': self._cache_misses,
            'hit_rate': hit_rate
        }
